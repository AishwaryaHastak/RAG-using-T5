{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amod/mental_health_counseling_conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'KonradSzafer/stackoverflow_python_preprocessed'\n",
    "dataset_name = 'graycatHCO3/CodeAlpaca-20K-Python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\h4has\\anaconda3\\envs\\rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 541/541 [00:00<?, ?B/s] \n",
      "Downloading data: 100%|██████████| 780k/780k [00:00<00:00, 2.04MB/s]\n",
      "Downloading data: 100%|██████████| 90.5k/90.5k [00:00<00:00, 268kB/s]\n",
      "Generating train split: 100%|██████████| 4777/4777 [00:00<00:00, 77436.35 examples/s]\n",
      "Generating test split: 100%|██████████| 548/548 [00:00<00:00, 182187.59 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 4777\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'completion'],\n",
      "        num_rows: 548\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(dataset_name)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'completion'],\n",
      "    num_rows: 4777\n",
      "})\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'])\n",
    "print(type(dataset['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = [data for data in dataset['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4777\n"
     ]
    }
   ],
   "source": [
    "print(len(data_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Write a reusuable function in Python that takes two string variables and returns the longest string.\\n', 'completion': 'def longest_string(str1, str2):\\n    if len(str1) > len(str2):\\n        return str1\\n    else:\\n        return str2'}\n"
     ]
    }
   ],
   "source": [
    "for data in dataset['train']:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "# model = 't5-small'\n",
    "model = 'flan-t5-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LlamaTokenizer, Llama\n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(model)\n",
    "# model = LlamaForCausalLM.from_pretrained(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is mental health?\"\n",
    "# inputs = tokenizer(query, return_tensors=\"pt\")\n",
    "# print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\h4has\\anaconda3\\envs\\rag\\lib\\site-packages\\transformers\\generation\\utils.py:1259: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  8477, 18154,     3,   390,  2869,    54,   199,    25,  1591,\n",
      "             6,    36,   213,    25,    33,     3,     6,  4839,    11, 16363]])\n",
      "Mindfulness based practices can help you ground, be where you are, relax and regulate\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outputs = model.generate(**inputs)\n",
    "print(outputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_dict)\n",
    "df.head(3)\n",
    "df.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136109\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.head(3)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer      1\n",
       "question    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreiving\n",
    "# Indexing - Elasticsearch, TF-IDF \n",
    "# Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minisearch\n",
    "ms = minisearch.Index(\n",
    "    text_fields=[\"question\", \"answer\"],\n",
    "    keyword_fields=[\"title\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minisearch.Index at 0x24068d0d370>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create index - minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Write a reusuable function in Python that takes two string variables and returns the longest string.\\n',\n",
       " 'completion': 'def longest_string(str1, str2):\\n    if len(str1) > len(str2):\\n        return str1\\n    else:\\n        return str2'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\h4has\\Documents\\Projects\\llm-zoomcamp\\src\\minisearch.py:51\u001b[0m, in \u001b[0;36mIndex.fit\u001b[1;34m(self, docs)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_fields:\n\u001b[0;32m     50\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mget(field, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_matrices[field] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorizers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeyword_fields:\n",
      "File \u001b[1;32mc:\\Users\\h4has\\anaconda3\\envs\\rag\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2084\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2086\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2087\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2088\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2089\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2090\u001b[0m )\n\u001b[1;32m-> 2091\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2093\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2094\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\h4has\\anaconda3\\envs\\rag\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\h4has\\anaconda3\\envs\\rag\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1372\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1364\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1365\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1368\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1369\u001b[0m             )\n\u001b[0;32m   1370\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1372\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1375\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\h4has\\anaconda3\\envs\\rag\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1278\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1279\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m         )\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "ms.fit(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search using index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"How to create a dictionary in python?\"\n",
    "query = \"How to sort a dictionary in python?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "results = ms.search(\n",
    "    query=query,\n",
    "    # filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "    # boost_dict=boost,\n",
    "    num_results=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why not try this approach. Let us define a dictionary called mydict with the following data:\\nmydict = {\\'carl\\':40,\\n          \\'alan\\':2,\\n          \\'bob\\':1,\\n          \\'danny\\':3}\\n\\nIf one wanted to sort the dictionary by keys, one could do something like:\\nfor key in sorted(mydict.iterkeys()):\\n    print \"%s: %s\" % (key, mydict[key])\\n\\nThis should return the following output:\\nalan: 2\\nbob: 1\\ncarl: 40\\ndanny: 3\\n\\nOn the other hand, if one wanted to sort a dictionary by value (as is asked in the question), one could do the following:\\nfor key, value in sorted(mydict.iteritems(), key=lambda (k,v): (v,k)):\\n    print \"%s: %s\" % (key, value)\\n\\nThe result of this command (sorting the dictionary by value) should return the following:\\nbob: 1\\nalan: 2\\ndanny: 3\\ncarl: 40\\n\\n',\n",
       " 'You can use the collections.Counter. Note, this will work for both numeric and non-numeric values.\\n>>> x = {1: 2, 3: 4, 4:3, 2:1, 0:0}\\n>>> from collections import Counter\\n>>> #To sort in reverse order\\n>>> Counter(x).most_common()\\n[(3, 4), (4, 3), (1, 2), (2, 1), (0, 0)]\\n>>> #To sort in ascending order\\n>>> Counter(x).most_common()[::-1]\\n[(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)]\\n>>> #To get a dictionary sorted by values\\n>>> from collections import OrderedDict\\n>>> OrderedDict(Counter(x).most_common()[::-1])\\nOrderedDict([(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)])\\n\\n']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = [result['answer'] for result in results]\n",
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a python coder. Provide concise and short python code for the question based on the context.\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{response}\n",
    "If you dont know the answer, just say that you dont know.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Sort a Python dictionary by value',\n",
       " 'answer': 'Why not try this approach. Let us define a dictionary called mydict with the following data:\\nmydict = {\\'carl\\':40,\\n          \\'alan\\':2,\\n          \\'bob\\':1,\\n          \\'danny\\':3}\\n\\nIf one wanted to sort the dictionary by keys, one could do something like:\\nfor key in sorted(mydict.iterkeys()):\\n    print \"%s: %s\" % (key, mydict[key])\\n\\nThis should return the following output:\\nalan: 2\\nbob: 1\\ncarl: 40\\ndanny: 3\\n\\nOn the other hand, if one wanted to sort a dictionary by value (as is asked in the question), one could do the following:\\nfor key, value in sorted(mydict.iteritems(), key=lambda (k,v): (v,k)):\\n    print \"%s: %s\" % (key, value)\\n\\nThe result of this command (sorting the dictionary by value) should return the following:\\nbob: 1\\nalan: 2\\ndanny: 3\\ncarl: 40\\n\\n',\n",
       " 'question': 'I have a dictionary of values read from two fields in a database: a string field and a numeric field. The string field is unique, so that is the key of the dictionary.\\nI can sort on the keys, but how can I sort based on the values?\\nNote: I have read Stack Overflow question How do I sort a list of dictionaries by values of the dictionary in Python? and probably could change my code to have a list of dictionaries, but since I do not really need a list of dictionaries I wanted to know if there is a simpler solution.\\n'}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(question=query, response=answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You\\'re a python coder. Provide concise and short python code for the question based on the context.\\nQUESTION: How to sort a dictionary in python?\\n\\nCONTEXT: \\n[\\'Why not try this approach. Let us define a dictionary called mydict with the following data:\\\\nmydict = {\\\\\\'carl\\\\\\':40,\\\\n          \\\\\\'alan\\\\\\':2,\\\\n          \\\\\\'bob\\\\\\':1,\\\\n          \\\\\\'danny\\\\\\':3}\\\\n\\\\nIf one wanted to sort the dictionary by keys, one could do something like:\\\\nfor key in sorted(mydict.iterkeys()):\\\\n    print \"%s: %s\" % (key, mydict[key])\\\\n\\\\nThis should return the following output:\\\\nalan: 2\\\\nbob: 1\\\\ncarl: 40\\\\ndanny: 3\\\\n\\\\nOn the other hand, if one wanted to sort a dictionary by value (as is asked in the question), one could do the following:\\\\nfor key, value in sorted(mydict.iteritems(), key=lambda (k,v): (v,k)):\\\\n    print \"%s: %s\" % (key, value)\\\\n\\\\nThe result of this command (sorting the dictionary by value) should return the following:\\\\nbob: 1\\\\nalan: 2\\\\ndanny: 3\\\\ncarl: 40\\\\n\\\\n\\', \\'You can use the collections.Counter. Note, this will work for both numeric and non-numeric values.\\\\n>>> x = {1: 2, 3: 4, 4:3, 2:1, 0:0}\\\\n>>> from collections import Counter\\\\n>>> #To sort in reverse order\\\\n>>> Counter(x).most_common()\\\\n[(3, 4), (4, 3), (1, 2), (2, 1), (0, 0)]\\\\n>>> #To sort in ascending order\\\\n>>> Counter(x).most_common()[::-1]\\\\n[(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)]\\\\n>>> #To get a dictionary sorted by values\\\\n>>> from collections import OrderedDict\\\\n>>> OrderedDict(Counter(x).most_common()[::-1])\\\\nOrderedDict([(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)])\\\\n\\\\n\\']\\nIf you dont know the answer, just say that you dont know.'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1591"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  148,    31,    60,     3,     9,     3,   102,    63,   189,   106,\n",
      "          1081,    52,     5,  7740, 22874,    11,   710,     3,   102,    63,\n",
      "           189,   106,  1081,    21,     8,   822,     3,   390,    30,     8,\n",
      "          2625,     5,     3, 15367,   134,  9562,    10,   571,    12,  1843,\n",
      "             3,     9, 24297,    16,     3,   102,    63,   189,   106,    58,\n",
      "          8472,  3463,     4,   382,    10,   784,    31, 17891,    59,   653,\n",
      "            48,  1295,     5,  1563,   178,  6634,     3,     9, 24297,   718,\n",
      "            82, 12194,    28,     8,   826,   331,    10,     2,    29,  2258,\n",
      "         12194,  3274,     3,     2,    31,  1720,    40,     2,    31,    10,\n",
      "          2445,     6,     2,    29,     3,     2,    31,     9,  1618,     2,\n",
      "            31,    10,  4482,     2,    29,     3,     2,    31, 17396,     2,\n",
      "            31,    10,  4347,     2,    29,     3,     2,    31,    26, 15159,\n",
      "             2,    31,    10,   519,     2,    29,     2,    29,  5801,    80,\n",
      "          1114,    12,  1843,     8, 24297,    57,  9060,     6,    80,   228,\n",
      "           103,   424,   114,    10,     2,    29,  1161,   843,    16,     3,\n",
      "         14504,   599,  2258, 12194,     5,   155,    49,  4397,     7,  9960,\n",
      "            61,    10,     2,    29,  2281,    96,  1454,     7,    10,     3,\n",
      "          1454,     7,   121,     3,  1454,    41,  4397,     6,    82, 12194,\n",
      "          6306,  4397,   908,    61,     2,    29,     2,    29,  3713,   225,\n",
      "          1205,     8,   826,  3911,    10,     2,    29,     9,  1618,    10,\n",
      "           204,     2,    29, 17396,    10,   209,     2,    29,  1720,    40,\n",
      "            10,  1283,     2,   727, 15159,    10,   220,     2,    29,     2,\n",
      "            29,  7638,     8,   119,   609,     6,     3,    99,    80,  1114,\n",
      "            12,  1843,     3,     9, 24297,    57,   701,    41,     9,     7,\n",
      "            19,  1380,    16,     8,   822,   201,    80,   228,   103,     8,\n",
      "           826,    10,     2,    29,  1161,   843,     6,   701,    16,     3,\n",
      "         14504,   599,  2258, 12194,     5,   155,    15,  5730,    51,     7,\n",
      "          9960,     6,   843,  2423,    40,   265,   115,    26,     9,    41,\n",
      "           157,     6,   208,    61,    10,    41,   208,     6,   157,    61,\n",
      "            61,    10,     2,    29,  2281,    96,  1454,     7,    10,     3,\n",
      "          1454,     7,   121,     3,  1454,    41,  4397,     6,   701,    61,\n",
      "             2,    29,     2,    29,   634,   741,    13,    48,  4106,    41,\n",
      "          9309,    53,     8, 24297,    57,   701,    61,   225,  1205,     8,\n",
      "           826,    10,     2,    29, 17396,    10,   209,     2,    29,     9,\n",
      "          1618,    10,   204,     2,   727, 15159,    10,   220,     2,    29,\n",
      "          1720,    40,    10,  1283,     2,    29,     2,    29,    31,     6,\n",
      "             3,    31,  3774,    54,   169,     8,  8274,     5, 10628,    49,\n",
      "             5,  2507,     6,    48,    56,   161,    21,   321,   206, 17552,\n",
      "            11,   529,    18,    29,    76, 17552,  2620,     5,     2,    29,\n",
      "          3155,  3155,  3155,     3,   226,  3274,     3,     2,   536,    10,\n",
      "          3547,   220,    10,  6464,   314,    10,  6355, 27078,     6,     3,\n",
      "           632,    10,   632,     2,    29,  3155,  3155,  3155,    45,  8274,\n",
      "          4830, 17706,     2,    29,  3155,  3155,  3155,  1713,  3696,  1843,\n",
      "            16,  7211,   455,     2,    29,  3155,  3155,  3155, 17706,   599,\n",
      "           226,   137,  5463,   834,   287,  2157,  9960,     2,    29,  6306,\n",
      "           599,  6355,   314,   201,  8457,     6,   220,   201,  4077,     6,\n",
      "          9266,     6,  4743,     6,  8925,     6, 17482,     6,     3,   632,\n",
      "            61,   908,     2,    29,  3155,  3155,  3155,  1713,  3696,  1843,\n",
      "            16, 25200,    53,   455,     2,    29,  3155,  3155,  3155, 17706,\n",
      "           599,   226,   137,  5463,   834,   287,  2157,  9960,  6306,    10,\n",
      "            10,  2292,   908,     2,    29,  6306,   599,   632,     6,     3,\n",
      "           632,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True, padding='max_length')\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     3, 15367,   134,  9562,    10,   784,    31, 17891,    59,\n",
      "           653,    48,  1295,     5,  1563,   178,  6634,     3,     9, 24297,\n",
      "           718,    82, 12194,    28,     8,   826,   331,    10,     2,    29,\n",
      "          2258, 12194,  3274,     3,     2,    31,  1720,    40,     2,    31,\n",
      "            10,  2445,     6,     2,    29,     3,     2,    31,     9,  1618,\n",
      "             2,    31,    10,  4482,     2,    29,     3,     2,    31, 17396,\n",
      "             2,    31,    10,  4347,     2,    29,     3,     2,    31,    26,\n",
      "         15159,     2,    31,    10,   519,     2,    29,     2,    29,   634,\n",
      "           741,    13,    48,  4106,    41,  9309,    53,     8, 24297,    57,\n",
      "           701,    61,   225,  1205,     8,   826,  3911,    10,     2,    29,\n",
      "         17396,    10,   209,     2,    29,     9,  1618,    10,   204,     2,\n",
      "           727, 15159,    10,   220,     1]])\n",
      "QUESTION: ['Why not try this approach. Let us define a dictionary called mydict with the following data:nmydict = 'carl':40,n 'alan':2,n 'bob':1,n 'danny':3nnThe result of this command (sorting the dictionary by value) should return the following output:nbob: 1nalan: 2ndanny: 3\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    inputs['input_ids'], \n",
    "    max_length=150, \n",
    "    num_beams=4, \n",
    "    # early_stopping=True\n",
    ")\n",
    "print(outputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling longer chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreiving\n",
    "# Indexing - Elasticsearch, TF-IDF \n",
    "# Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minisearch\n",
    "\n",
    "\n",
    "ms = minisearch.Index(\n",
    "    text_fields=[\"Response\"],\n",
    "    keyword_fields=[\"Context\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minisearch.Index at 0x1a9a25371c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.fit(data_dict)"
=======
    "### Prpmpt"
>>>>>>> 5cf7592f315d2a7deff2e42aa37c82a6ee2a3bd9
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, max_length=512):\n",
    "    # Split text into chunks of max_length tokens\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")['input_ids'][0]\n",
    "    chunks = [tokens[i:i+max_length] for i in range(0, len(tokens), max_length)]\n",
    "    return chunks\n",
    "\n",
    "# Split long text\n",
    "text_chunks = split_text(long_text)\n",
    "\n",
    "# Generate output for each chunk\n",
    "results = []\n",
    "for chunk in text_chunks:\n",
    "    inputs = tokenizer.decode(chunk, return_tensors=\"pt\").unsqueeze(0)\n",
    "    outputs = model.generate(inputs['input_ids'], max_length=150, num_beams=4, early_stopping=True)\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    results.append(decoded_output)\n",
    "\n",
    "# Combine results if necessary\n",
    "combined_output = ' '.join(results)\n",
    "print(combined_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
<<<<<<< HEAD
   "source": []
=======
   "source": [
    "Amod/mental_health_counseling_conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\h4has\\anaconda3\\envs\\rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Context', 'Response'],\n",
      "        num_rows: 3512\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"Amod/mental_health_counseling_conversations\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Context', 'Response'],\n",
      "    num_rows: 3512\n",
      "})\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'])\n",
    "print(type(dataset['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = [data for data in dataset['train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3512\n"
     ]
    }
   ],
   "source": [
    "print(len(data_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Context': \"I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\\n   How can I change my feeling of being worthless to everyone?\", 'Response': \"If everyone thinks you're worthless, then maybe you need to find new people to hang out with.Seriously, the social context in which a person lives is a big influence in self-esteem.Otherwise, you can go round and round trying to understand why you're not worthless, then go back to the same crowd and be knocked down again.There are many inspirational messages you can find in social media. \\xa0Maybe read some of the ones which state that no person is worthless, and that everyone has a good purpose to their life.Also, since our culture is so saturated with the belief that if someone doesn't feel good about themselves that this is somehow terrible.Bad feelings are part of living. \\xa0They are the motivation to remove ourselves from situations and relationships which do us more harm than good.Bad feelings do feel terrible. \\xa0 Your feeling of worthlessness may be good in the sense of motivating you to find out that you are much better than your feelings today.\"}\n"
     ]
    }
   ],
   "source": [
    "for data in dataset['train']:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "model = 't5-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LlamaTokenizer, Llama\n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(model)\n",
    "# model = LlamaForCausalLM.from_pretrained(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What is mental health?\"\n",
    "# inputs = tokenizer(query, return_tensors=\"pt\")\n",
    "# print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0, 2751,  229, 2550,  533,   58,    1]])\n",
      "Was ist mental health?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\h4has\\anaconda3\\envs\\rag\\lib\\site-packages\\transformers\\generation\\utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outputs = model.generate(**inputs)\n",
    "print(outputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>If everyone thinks you're worthless, then mayb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>Hello, and thank you for your question and see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>First thing I'd suggest is getting the sleep y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Context  \\\n",
       "0  I'm going through some things with my feelings...   \n",
       "1  I'm going through some things with my feelings...   \n",
       "2  I'm going through some things with my feelings...   \n",
       "\n",
       "                                            Response  \n",
       "0  If everyone thinks you're worthless, then mayb...  \n",
       "1  Hello, and thank you for your question and see...  \n",
       "2  First thing I'd suggest is getting the sleep y...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreiving\n",
    "# Indexing - Elasticsearch, TF-IDF \n",
    "# Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minisearch\n",
    "\n",
    "\n",
    "ms = minisearch.Index(\n",
    "    text_fields=[\"Response\"],\n",
    "    keyword_fields=[\"Context\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minisearch.Index at 0x1a9e38cc8e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create index - minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minisearch.Index at 0x1a9e38cc8e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms.fit(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search using index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"It is very scary for me to socialize with unknown people\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "results = ms.search(\n",
    "    query=query,\n",
    "    # filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "    boost_dict=boost,\n",
    "    num_results=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Context': \"Sometimes I can't stop thinking about life after death. I was raised in a religion that teaches that we will live on forever either in hell or in heaven. \\r\\nWhen I think of living forever (even if it is in heaven which should be good), I feel overwhelmed. I don't like the thought of living forever and ever and ever. Sometimes I just can't get the thought out of my mind and the thoughts lead to panic and anxiety. \\r\\nAm I crazy? I don't think these thoughts are normal.\",\n",
       "  'Response': \"You might be surprised how normal you are. Anxiety is incredibly common and while your particular type of existential anxiety might be unique to you - it is very difficult for most people to really comprehend what happens after we die - regardless of the religious or philosophical belief systems we hold. It is the ultimate unknown and some philosophers and psychologists believe that at the root of our day-to-day anxieties is the fear of death or fear of the unknown. \\xa0Just as it can be really hard to comprehend the ending of life it can also be hard to comprehend an eternal existence. What these both have in common is that we are imaging a future that is ultimately unknowable and this unknown can provoke a lot of anxiety.\\xa0Mindfulness based practices like meditation - maybe there is something like this in your religious tradition - can be very helpful in making peace with the unknown in the present moment. The more we can learn to live in the moment - the less we get hung up on anticipating outcomes for our lives that may never come true. Mindfulness practices can help you ground, be where you are , relax and regulate your nervous system so that you are able sleep and recuperate, and train your attention to focus on living the life you want to live now - rather than worrying about what happens after you die.\\xa0Having said all that - it can be profoundly helpful to speak with someone about your anxiety - especially when you feel haunted by it, worry that you are crazy and can't get to sleep. There are lots of good therapists out there who can help you with your anxiety.\\xa0\"},\n",
       " {'Context': \"Sometimes I can't stop thinking about life after death. I was raised in a religion that teaches that we will live on forever either in hell or in heaven.  When I think of living forever (even if it is in heaven which should be good), I feel overwhelmed. I don't like the thought of living forever and ever and ever. Sometimes I just can't get the thought out of my mind and the thoughts lead to panic and anxiety.  Am I crazy? I don't think these thoughts are normal.\",\n",
       "  'Response': \"You might be surprised how normal you are. Anxiety is incredibly common and while your particular type of existential anxiety might be unique to you - it is very difficult for most people to really comprehend what happens after we die - regardless of the religious or philosophical belief systems we hold. It is the ultimate unknown and some philosophers and psychologists believe that at the root of our day-to-day anxieties is the fear of death or fear of the unknown. \\xa0Just as it can be really hard to comprehend the ending of life it can also be hard to comprehend an eternal existence. What these both have in common is that we are imaging a future that is ultimately unknowable and this unknown can provoke a lot of anxiety.\\xa0Mindfulness based practices like meditation - maybe there is something like this in your religious tradition - can be very helpful in making peace with the unknown in the present moment. The more we can learn to live in the moment - the less we get hung up on anticipating outcomes for our lives that may never come true. Mindfulness practices can help you ground, be where you are , relax and regulate your nervous system so that you are able sleep and recuperate, and train your attention to focus on living the life you want to live now - rather than worrying about what happens after you die.\\xa0Having said all that - it can be profoundly helpful to speak with someone about your anxiety - especially when you feel haunted by it, worry that you are crazy and can't get to sleep. There are lots of good therapists out there who can help you with your anxiety.\"},\n",
       " {'Context': \"I've always thought that there wasn't much good out there for me. Now that things are actually going well, it kind of scares me. I spent most of my life feeling unwanted and figured I would be alone. I recently met a great woman who seems to really like me, and I don't know how to process this. It's bothering both of us.\",\n",
       "  'Response': \"Hi! Thank you for your question. It's tough to be in a healthy and loving relationship when we believe we are not worth it. Quite often being in such a relationship is very uncomfortable as it goes against everything we believe about ourselves.\\xa0First, I want to say you are very brave to open up yourself to the relationship with this wonderful woman you met.\\xa0Second, I would like to invite you to treat yourself with compassion as you are entering this unknown territory for yourself. Doing something new and totally unfamiliar can be scary, so it's vital that you are kind and gentle with yourself. Acknowledge that what you are doing is scary and uncomfortable and that it will take some time to get used to it.\\xa0Don't judge yourself and force yourself to feel a certain way. It's important that you are patient with yourself and tolerant of your discomfort. With time, space, kindness, and tolerance your discomfort and fear will start to dissolve. Invite your new partner to also relate to your feelings in the open and compassionate way.\\xa0The worst thing that you or your partner can do is to rush your feelings, pressure you to feel in a different way or to say that there is something wrong with you cause you feel a certain way. Please be gentle with yourself and celebrate and acknowledge every small victory. Every time you do something little, like holding hands or receive a compliment and it no longer scares you as it did a week ago - celebrate that progress!\"}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a mental health assistant. Provide friendly answers based on the context.\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{response}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(question=query, response=results[0]['Response'])"
   ]
>>>>>>> 5cf7592f315d2a7deff2e42aa37c82a6ee2a3bd9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
